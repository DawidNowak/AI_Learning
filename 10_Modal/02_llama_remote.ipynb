{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import modal\n",
    "from modal import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "\n",
    "app = modal.App(\"llama\")\n",
    "image = Image.debian_slim().pip_install(\"torch\", \"transformers\", \"bitsandbytes\", \"accelerate\")\n",
    "secrets = [modal.Secret.from_name(\"huggingface-secret\")]\n",
    "GPU = \"T4\"\n",
    "MODEL_NAME = \"meta-llama/Meta-Llama-3.1-8B\" # \"google/gemma-2-2b\"\n",
    "\n",
    "@app.function(image=image, secrets=secrets, gpu=GPU, timeout=1800)\n",
    "def generate(prompt: str) -> str:\n",
    "    import torch\n",
    "    from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, set_seed\n",
    "\n",
    "    # Quant Config\n",
    "    quant_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        bnb_4bit_quant_type=\"nf4\"\n",
    "    )\n",
    "\n",
    "    # Load model and tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"right\"\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME, \n",
    "        quantization_config=quant_config,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "\n",
    "    set_seed(42)\n",
    "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    attention_mask = torch.ones(inputs.shape, device=\"cuda\")\n",
    "    outputs = model.generate(inputs, attention_mask=attention_mask, num_return_sequences=1)\n",
    "    return tokenizer.decode(outputs[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ephemeral app - the app is built and deployed to the Modal platform,\n",
    "# Llama model will be pulled from Hugging Face and the function will be executed\n",
    "# then the app will be stopped and the resources will be released\n",
    "\n",
    "with modal.enable_output():\n",
    "    with app.run():\n",
    "        result=generate.remote(\"Tell a lighthearted joke about programmers.\")\n",
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
